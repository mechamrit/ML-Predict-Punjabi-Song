{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WebScraperV2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mechamrit/ML-Predict-Punjabi-Song/blob/master/WebScraperV2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-8aGt0mbBMG",
        "colab_type": "text"
      },
      "source": [
        "# Web Scrapper\n",
        "Web Scrapper is used to automated script to get data from whe website. We can see whether its allowed or not in root.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tRgbYLWc0tr",
        "colab_type": "text"
      },
      "source": [
        "## Importing Libraries\n",
        "Importing requests to get the website from url and bs4 is used to append and play with html. Refer documentation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XRHNk8lbAlr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Importing Libraries\n",
        "# Requests for fetching html of website\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KnZUMif4dchW",
        "colab_type": "text"
      },
      "source": [
        "## Parameters\n",
        "url of the website </br>\n",
        "Filename in which it will store the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDiWzPW9cAvw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Parameters \n",
        "## Website link is not provided due to honor of code\n",
        "url = \"url of site\"\n",
        "filename = \"test.txt\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4r9BaLZb6J_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Make the request to a url\n",
        "r = requests.get(url)\n",
        "c = r.content\n",
        "#print(c)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iU5bPnmpb7wZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create soup from content of request\n",
        "soup = BeautifulSoup(c,'html.parser')\n",
        "# print(soup.prettify())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvmmxMocdqWi",
        "colab_type": "text"
      },
      "source": [
        "## Script\n",
        "Script is made according to the website to extract the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFifVYExbxYm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_song(link):\n",
        "  r1 = requests.get(flink).content\n",
        "  new_soup = BeautifulSoup(r1,'html.parser')\n",
        "  song_html = new_soup.find(\"div\",attrs={\"class\":\"text-base lg:text-lg pb-2\"})\n",
        "  lyrics = str(song_html)\n",
        "  lyrics1 = lyrics.replace(\"<br/>\",\" \\n \")\n",
        "  new_soup1 = BeautifulSoup(lyrics1,'html.parser')\n",
        "  lyrics2 = new_soup1.text\n",
        "  f.write(lyrics2)\n",
        "  print(lyrics2)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGaKpTgwafJB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "f = open(\"filename\", \"w\")\n",
        "for page in range (1,5):\n",
        "  r = requests.get(\"url/punjabi?page=\"+str(page)+\".html\")\n",
        "  c = r.content\n",
        "  soup = BeautifulSoup(c,'html.parser')\n",
        "  container = soup.find(\"div\",attrs={\"class\":\"container mx-auto p-5 pt-0 flex flex-col md:flex-row\"})\n",
        "  containers = soup.find_all(\"a\",attrs={\"class\":\"h-auto w-full block text-black no-underline\"})\n",
        "  soup_string = str(containers)\n",
        "  # print(containers[0])\n",
        "\n",
        "  new = \"\"\n",
        "  for x in soup_string: \n",
        "    new = new + x\n",
        "  final_soup = BeautifulSoup(new,'html.parser')\n",
        "  final_soup\n",
        " \n",
        "  # containers.find(\"div\")\n",
        "  for link in final_soup.find_all('a'):\n",
        "      link = (link.get('href'))\n",
        "      flink = \"url\"+link+'.html'\n",
        "      get_song(flink)\n",
        "    \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}